{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import html\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata as ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PreProcess:\n",
    "    '''This class contains all text pre-processing function\n",
    "    # Input parameters: Dataframe, Column_name on which function needs to be applied\n",
    "    # Output parameters: Return dataframe after applying operations\n",
    "    '''\n",
    "    # todo: Pass functions as a list of arguments to apply in the class\n",
    "    # todo: make set of words before applying all operations to reduce processing time.\n",
    "    def __init__(self, data, column_name):\n",
    "        self.data = data\n",
    "        self.column_name = column_name\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatiser = WordNetLemmatizer()\n",
    "        # pass\n",
    "\n",
    "    def remove_non_arabic(self):\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \"\".join(i for i in x if ((ord(i) > 1536 and ord(i) < 1791) or ord(i) == 32)))\n",
    "        return self.data\n",
    "\n",
    "    def remove_arabic_numbers_puncts(self):\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: ''.join(c for c in x if not ud.category(c).startswith('P')))\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: ''.join(c for c in x if not ud.category(c).startswith('Nd')))\n",
    "        return self.data\n",
    "        \n",
    "\n",
    "    def clean_html(self):\n",
    "        \"\"\"remove html entities\"\"\"\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(html.unescape)\n",
    "        return self.data\n",
    "\n",
    "    def remove_spaces(self):\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: x.replace('\\n', ' '))\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: x.replace('\\t', ' '))\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: x.replace('  ', ' '))\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: x.lower())\n",
    "        return self.data\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        tr = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        # self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join([item.translate(tr)\n",
    "        #                                                                 for item in x.split()]))\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: x.translate(tr))\n",
    "        return self.data\n",
    "\n",
    "    def stemming(self):\n",
    "        # todo: provide option of selecting stemmer.\n",
    "        snowball_stemmer = SnowballStemmer('arabic')\n",
    "        # self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join([snowball_stemmer.stem(item)\n",
    "        #                                                                 for item in x.split()]))\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join([self.stemmer.stem(item)\n",
    "                                                                        for item in x.split()]))\n",
    "        return self.data\n",
    "\n",
    "    def lemmatization(self):\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join([self.lemmatiser.lemmatize(item)\n",
    "                                                                        for item in x.split()]))\n",
    "        return self.data\n",
    "\n",
    "    def stop_words(self):\n",
    "        stop = stopwords.words('arabic')\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda x: \" \".join(set([item for item in x.split() if\n",
    "                                                                                       item not in stop])))\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The bot or not dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sl Number                                         Tweet Text  Is_Bot\n",
      "0          1  الوطن حكومةً وشعبًا يفتخر ويعتز بــ #نجاح_موسم...       0\n",
      "1          2  اللهم صل على سيدنا محمد #Bahrain #البحرين #الس...       0\n",
      "2          3  #محمد_بن_سلمان_يهدم_ايران من أقوال محمد بن سلم...       0\n",
      "3          4  #اعجبتني #السعوديه_اليابان #السعودية #جبس #ديك...       0\n",
      "4          5  وزير النفط: قريباً.. عودة الإنتاج بالمنطقة الم...       0\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('/home/ashhadulislam/projects/hbku/fall_2019/app_deep_learning/nlp_arabic/data/QICC_Bot_OR_Not/train.csv')\n",
    "print(df.head())\n",
    "pre_processor = PreProcess(df, \"Tweet Text\")\n",
    "#the pre process class is defined at the end. Run that first\n",
    "data = pre_processor.clean_html()\n",
    "data = pre_processor.remove_non_arabic()\n",
    "data = pre_processor.remove_arabic_numbers_puncts()\n",
    "data = pre_processor.remove_spaces()\n",
    "data = pre_processor.remove_punctuation()\n",
    "data = pre_processor.stemming()\n",
    "data = pre_processor.lemmatization()\n",
    "data = pre_processor.stop_words()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"/home/ashhadulislam/projects/hbku/fall_2019/app_deep_learning/nlp_arabic/data/QICC_Bot_OR_Not/train_pre_processed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sl No                                         Tweet Text  Is_Bot\n",
      "0      1  هذه طائرة تدريب في #السعودية للشباب الصغار ......       0\n",
      "1      2  د. عبدالرحمن العناد: تدرك ميليشيات الحوثي أن ا...       1\n",
      "2      3  د. محمد الثقفي: نجاح حج هذا العام وسلامة ضيوف ...       1\n",
      "3      4  سلطة بابا غنوج baba ganoush #حلا #كيك #معجنات ...       0\n",
      "4      5  #السعودية.. النيابة تطالب بإعدام الشيخ #سلمان_...       0\n"
     ]
    }
   ],
   "source": [
    "df_super_test=pd.read_csv(\"/home/ashhadulislam/projects/hbku/fall_2019/app_deep_learning/nlp_arabic/data/QICC_Bot_OR_Not/test.csv\")\n",
    "print(df_super_test.head())\n",
    "pre_processor = PreProcess(df_super_test, \"Tweet Text\")\n",
    "#the pre process class is defined at the end. Run that first\n",
    "data_super_test = pre_processor.clean_html()\n",
    "data_super_test = pre_processor.remove_non_arabic()\n",
    "data_super_test = pre_processor.remove_arabic_numbers_puncts()\n",
    "data_super_test = pre_processor.remove_spaces()\n",
    "data_super_test = pre_processor.remove_punctuation()\n",
    "data_super_test = pre_processor.stemming()\n",
    "data_super_test = pre_processor.lemmatization()\n",
    "data_super_test = pre_processor.stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_super_test.to_csv(\"/home/ashhadulislam/projects/hbku/fall_2019/app_deep_learning/nlp_arabic/data/QICC_Bot_OR_Not/test_pre_processed.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    925\n",
       "1    690\n",
       "Name: Is_Bot, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_super_test.Is_Bot.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app_dl",
   "language": "python",
   "name": "app_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
